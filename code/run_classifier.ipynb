{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testEntities_crf='consolidated2/consolidated/dev+test+context.new.crf.p'\n",
    "trainEntities_crf='consolidated2/consolidated/train+context.new.crf.p'\n",
    "\n",
    "testEntities_ent='consolidated/dev+test+context.5.p'\n",
    "trainEntities_ent='consolidated/train+context.5.p'\n",
    "\n",
    "train_articles_crf, train_titles_crf, train_identifiers_crf, train_downloaded_articles_crf, \\\n",
    "TRAIN_ENTITIES_CRF, TRAIN_CONFIDENCES_CRF, TRAIN_COSINE_SIM_CRF, CONTEXT1_crf, CONTEXT2_crf = pickle.load(open(trainEntities_crf, \"rb\"))\n",
    "\n",
    "#load cached entities (speed up)\n",
    "train_articles, train_titles, train_identifiers, train_downloaded_articles, \\\n",
    "TRAIN_ENTITIES, TRAIN_CONFIDENCES, TRAIN_COSINE_SIM, CONTEXT1, CONTEXT2 = pickle.load(open(trainEntities_ent, \"rb\"))\n",
    "\n",
    "\n",
    "test_articles_crf, test_titles_crf, test_identifiers_crf, test_downloaded_articles_crf,\\\n",
    "TEST_ENTITIES_CRF, TEST_CONFIDENCES_CRF, TEST_COSINE_SIM_CRF, CONTEXT_crf, CONTEXT2_crf = pickle.load(open(testEntities_crf, \"rb\"))\n",
    "\n",
    "test_articles, test_titles, test_identifiers, test_downloaded_articles,\\\n",
    "TEST_ENTITIES, TEST_CONFIDENCES, TEST_COSINE_SIM, CONTEXT, CONTEXT2 = pickle.load(open(testEntities_ent, \"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as MaxEnt\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "from itertools import izip\n",
    "import sys, json, pdb, pickle, operator, collections\n",
    "import predict as p\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "def dd():\n",
    "    return {}\n",
    "\n",
    "def ddd():\n",
    "    return collections.defaultdict(dd)\n",
    "\n",
    "class Classifier(object):\n",
    "\n",
    "    def __init__(self, TRAIN_ENTITIES, TRAIN_CONFIDENCES, TRAIN_COSINE_SIM,\\\n",
    "                 TEST_ENTITIES, TEST_CONFIDENCES, TEST_COSINE_SIM):\n",
    "        self.TRAIN_ENTITIES = TRAIN_ENTITIES\n",
    "        self.TRAIN_CONFIDENCES = TRAIN_CONFIDENCES\n",
    "        self.TRAIN_COSINE_SIM = TRAIN_COSINE_SIM\n",
    "\n",
    "        self.TEST_ENTITIES = TEST_ENTITIES\n",
    "        self.TEST_CONFIDENCES = TEST_CONFIDENCES\n",
    "        self.TEST_COSINE_SIM = TEST_COSINE_SIM\n",
    "\n",
    "        self.match_orig_feature = True\n",
    "        self.print_query_scores = False\n",
    "\n",
    "\n",
    "    ##current_ent_locations is an array of len 4[query, supp_article] to signify which artic\n",
    "    def trainClassifier(self, train_identifiers):\n",
    "\n",
    "        classifier = MaxEnt(solver=\"lbfgs\", verbose=1)\n",
    "        X = []\n",
    "        Y = []\n",
    "        num_neg = 0\n",
    "        max_neg = 1000\n",
    "        for article_index in range(len(self.TRAIN_ENTITIES)):\n",
    "            article = self.TRAIN_ENTITIES[article_index]\n",
    "            for query_index in range(len(article)):\n",
    "                query = article[query_index]\n",
    "                for supporting_article_index in range(1,len(query)):\n",
    "                    features = self.getFeatures(article_index, query_index, supporting_article_index, \\\n",
    "                                                self.TRAIN_ENTITIES, self.TRAIN_CONFIDENCES,self.TRAIN_COSINE_SIM, CONTEXT1)\n",
    "                    \n",
    "                    labels = self.getLabels(article_index, query_index, supporting_article_index, \\\n",
    "                                                self.TRAIN_ENTITIES, train_identifiers)\n",
    "                        \n",
    "                    for label in labels:\n",
    "                        if label == 5: \n",
    "                            if num_neg < max_neg:\n",
    "                                num_neg+=1 \n",
    "                                X.append(features)\n",
    "                                Y.append(label)\n",
    "                        else:\n",
    "                            X.append(features)\n",
    "                            Y.append(label)\n",
    "                assert( len(X) == len(Y))\n",
    "                \n",
    "        print \"Class dist\", [sum([y == i for y in Y])for i in range(6)]\n",
    "        print \"Total labels\", len(Y)\n",
    "        classifier.fit(X,Y)\n",
    "            \n",
    "        return classifier\n",
    "\n",
    "    def predictEntities(self, classifier):\n",
    "        \n",
    "#         print \"Classifier coef\", classifier.coef_\n",
    "#         print \"Classifeir intercept\", classifier.intercept_\n",
    "        predictions = [0,0,0,0,0,0]\n",
    "        DECISIONS = copy.deepcopy(self.TEST_ENTITIES)\n",
    "        i = 0\n",
    "        for article_index in range(len(self.TEST_ENTITIES)):\n",
    "            article = self.TEST_ENTITIES[article_index]\n",
    "            for query_index in range(len(article)):\n",
    "                query = article[query_index]\n",
    "                for supporting_article_index in range(len(query)):\n",
    "                    if supporting_article_index == 0:\n",
    "                        DECISIONS[article_index][query_index]\\\n",
    "                            [supporting_article_index] = [1, 1, 1, 1]\n",
    "                        continue\n",
    "                    DECISIONS[article_index][query_index]\\\n",
    "                            [supporting_article_index] = [0, 0, 0, 0] \n",
    "\n",
    "                    features = self.getFeatures(article_index, query_index, supporting_article_index, self.TEST_ENTITIES, self.TEST_CONFIDENCES,\\\n",
    "                               self.TEST_COSINE_SIM, CONTEXT2)\n",
    "\n",
    "#                     assert len(features) == 41\n",
    "                    prediction = classifier.predict(features)[0]\n",
    "                    predictions[prediction] += 1\n",
    "                    if prediction < 4:\n",
    "                        DECISIONS[article_index][query_index]\\\n",
    "                            [supporting_article_index][prediction] = 1\n",
    "                    elif prediction == 4:\n",
    "                        DECISIONS[article_index][query_index]\\\n",
    "                            [supporting_article_index] = [1, 1, 1, 1]\n",
    "                \n",
    "\n",
    "        return DECISIONS\n",
    "\n",
    "    #Run both Max Confidence and Majority Aggregation Schemes given the decisions\n",
    "    #Return the decided tag for each query\n",
    "    def aggregateResults(self, DECISIONS):\n",
    "        majority = []\n",
    "        max_conf = []\n",
    "        for article_index in range(len(self.TEST_ENTITIES)):\n",
    "            max_conf.append([])\n",
    "            majority.append([])\n",
    "            article = self.TEST_ENTITIES[article_index]\n",
    "            for query_index in range(len(article)):\n",
    "                max_conf[article_index].append([])\n",
    "                majority[article_index].append([])\n",
    "                query = article[query_index]\n",
    "                for entity_index in range(4):\n",
    "                    max_confidence = -1\n",
    "                    max_confidence_tag = ''\n",
    "                    tag_occurances = {}\n",
    "                    for supporting_article_index in range(len(query)):\n",
    "                        supporting_article = query[supporting_article_index]\n",
    "                        if DECISIONS[article_index][query_index][supporting_article_index]\\\n",
    "                           [entity_index] == 0:\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        confidence = self.TEST_CONFIDENCES[article_index][query_index]\\\n",
    "                                [supporting_article_index][entity_index]\n",
    "                        entity = supporting_article[entity_index].strip().lower()\n",
    "#                         assert(not entity == '')\n",
    "\n",
    "                        ##Update counts of majority\n",
    "                        if entity not in tag_occurances:\n",
    "                            tag_occurances[entity] = 1\n",
    "                        else:\n",
    "                            tag_occurances[entity] += 1\n",
    "\n",
    "                        ##Update max_confidence\n",
    "                        if confidence > max_confidence:\n",
    "                            max_confidence = confidence\n",
    "                            max_confidence_tag = entity\n",
    "                    max_majority_count = -1\n",
    "                    majority_tag = ''\n",
    "                    for ent in tag_occurances:\n",
    "                        if tag_occurances[ent] > max_majority_count:\n",
    "                            max_majority_count = tag_occurances[ent]\n",
    "                            majority_tag = ent\n",
    "                    max_conf[article_index][query_index].append(max_confidence_tag)\n",
    "                    majority[article_index][query_index].append(majority_tag)\n",
    "\n",
    "        return majority, max_conf\n",
    "\n",
    "\n",
    "\n",
    "    def evaluateBaseline(self, predicted_identifiers, test_identifiers, COUNT_ZERO):\n",
    "        for entity_index in range(4):\n",
    "            num_queries = 5\n",
    "            predicted_correct = [0.] * num_queries\n",
    "            total_predicted   = [0.] * num_queries\n",
    "            total_gold        = [0.] * num_queries\n",
    "\n",
    "            for article_index in range(len(predicted_identifiers)):\n",
    "                ## TODO: Add classifier for selecting query index?\n",
    "                for query_index in range(len(predicted_identifiers[article_index])):        \n",
    "                    predicted = predicted_identifiers[article_index][query_index][entity_index].strip().lower()\n",
    "                    gold = test_identifiers[article_index][entity_index].strip().lower()\n",
    "                    if gold == '' or (not COUNT_ZERO and gold == 'zero'):\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    #special handling for shooterName (lenient eval)\n",
    "                    if entity_index == 0:\n",
    "                        predicted = set(predicted.split('|'))\n",
    "                        gold = set(gold.split('|'))\n",
    "                        correct = gold.intersection(predicted)\n",
    "                        predicted_correct[query_index] += (1 if len(correct)>0 else 0)\n",
    "                        total_predicted[query_index] += 1\n",
    "                        total_gold[query_index] += 1 \n",
    "                    else:\n",
    "                        total_predicted[query_index] += 1\n",
    "                        if predicted == gold:\n",
    "                            predicted_correct[query_index] += 1\n",
    "                        total_gold[query_index] += 1\n",
    "\n",
    "\n",
    "            print \"Entity\", entity_index, \":\",\n",
    "            if sum(total_predicted) == 0 :\n",
    "                continue\n",
    "\n",
    "            if sum(predicted_correct) == 0 :\n",
    "                continue\n",
    "\n",
    "            if  self.print_query_scores:\n",
    "                print \"BEGINNING WITH PER QUERY SCORES\"\n",
    "\n",
    "                for query_index in range(num_queries):\n",
    "                    print \"*********************************************\"\n",
    "                    print\n",
    "                    print \"QUERY INDEX:\", query_index\n",
    "                    self.displayScore(predicted_correct[query_index], total_predicted[query_index],\\\n",
    "                                      total_gold[query_index])\n",
    "                    print\n",
    "                    print \"*********************************************\"\n",
    "                print \"NOW SHOWING SCORES AGGREGATED OVER ALL QUERRIES\"\n",
    "            self.displayScore(sum(predicted_correct), sum(total_predicted),sum(total_gold))\n",
    "\n",
    "    def displayScore(self, predicted_correct, total_predicted, total_gold):\n",
    "        precision = predicted_correct / total_predicted\n",
    "        recall = predicted_correct / total_gold\n",
    "        f1 = (2*precision*recall)/(precision+recall)\n",
    "        print\n",
    "        print \"PRECISION\", precision, \"RECALL\", recall, \"F1\", f1\n",
    "        print \"Total match\", predicted_correct\n",
    "\n",
    "    def trainAndEval(self, train_identifiers, test_identifiers, COUNT_ZERO):\n",
    "        classifier = self.trainClassifier(train_identifiers)\n",
    "        DECISIONS  = self.predictEntities(classifier)\n",
    "\n",
    "        debug = True\n",
    "        if debug:\n",
    "            self.runExploratoryTests(DECISIONS, train_identifiers, test_identifiers)\n",
    "            \n",
    "\n",
    "        majority, max_conf = self.aggregateResults(DECISIONS)\n",
    "        print \"#############################################################\"\n",
    "        print \"Evaluation for Classifier baseline with MAJORITY aggregation\"\n",
    "        print\n",
    "        self.evaluateBaseline(majority, test_identifiers, COUNT_ZERO)\n",
    "\n",
    "        print\n",
    "        print \"#############################################################\"\n",
    "        print \"Evaluation for Classifier baseline with MAX CONFIDENCE aggregation\"\n",
    "        print\n",
    "        self.evaluateBaseline(max_conf, test_identifiers, COUNT_ZERO)\n",
    "        print\n",
    "        print \"#############################################################\"\n",
    "    \n",
    "    def getFeatures(self, article_index, query_index, supporting_article_index, entities, confidences, cosine_sim, context):        \n",
    "        features= []\n",
    "\n",
    "        #Construct feature vector for this sampled entity\n",
    "        original_confidence = confidences[article_index][query_index][0]\n",
    "        confidence = confidences[article_index][query_index][supporting_article_index]\n",
    "        \n",
    "        #One hot vector to show if entity matches orginal\n",
    "        original_entity = entities[article_index][query_index][0]\n",
    "        new_entity = entities[article_index][query_index][supporting_article_index]\n",
    "        match_features = []\n",
    "        for e_index in range(len(original_entity)):\n",
    "            if original_entity[e_index] == '':\n",
    "                match_features += [0, 1]\n",
    "            elif original_entity[e_index].strip().lower() == new_entity[e_index].strip().lower():\n",
    "                match_features += [1, 0]\n",
    "            else:\n",
    "                match_features += [0, 1]\n",
    "        \n",
    "        # Cosine sim array is shifted by one.\n",
    "        # Index 0 should be 1 as orig is same as itself.\n",
    "        tfidf = 1 if supporting_article_index == 0 else \\\n",
    "                cosine_sim[article_index]\\\n",
    "                [query_index][supporting_article_index - 1]\n",
    "\n",
    "        features = original_confidence+ confidence + match_features + [tfidf]\n",
    "\n",
    "        for c in context[article_index][query_index][supporting_article_index]:\n",
    "            features += c            \n",
    "#          assert len(features) == 41\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def getLabels(self, article_index, query_index, supporting_article_index, entities, identifier):\n",
    "        #Extract out label for this article (ie. is label correct)\n",
    "        labels = []\n",
    "        gold_entities = identifier[article_index]\n",
    "        new_entities      = entities[article_index][query_index][supporting_article_index]\n",
    "        orig_entities     = entities[article_index][query_index][0]\n",
    "        for ind in range(len(gold_entities)):\n",
    "            ent = new_entities[ind].lower().strip()\n",
    "            orig_ent = orig_entities[ind].lower().strip()\n",
    "            gold = gold_entities[ind].lower().strip()\n",
    "            if gold == \"\":\n",
    "                continue\n",
    "            if ent == \"\":\n",
    "                continue\n",
    "            \n",
    "            #special handling for shooterName (entity_index = 0)\n",
    "            if ind == 0:\n",
    "                new_person = set(ent.split('|'))\n",
    "                gold_person = set(gold.split('|'))\n",
    "                if len(new_person.intersection(gold_person)) > 0:\n",
    "                    if not ent == orig_ent:\n",
    "                        labels.append(ind)\n",
    "            else:\n",
    "                if gold == ent:\n",
    "                    if not ent == orig_ent:\n",
    "                        labels.append(ind)\n",
    "        if labels == [0, 1, 2, 3]:\n",
    "            labels = [4]\n",
    "        elif labels == []:\n",
    "            labels = [5]\n",
    "        \n",
    "        assert (len(labels) > 0)\n",
    "        return labels\n",
    "    \n",
    "            \n",
    "    def runExploratoryTests(self, DECISIONS, train_identifiers, test_identifiers):\n",
    "        print \"TRAIN:: Exploring how many times gold entity is not in original document\"\n",
    "        count = collections.defaultdict(lambda:0.)\n",
    "        total_count = collections.defaultdict(lambda:0.)\n",
    "        for article_index in range(len(self.TRAIN_ENTITIES)):\n",
    "            article = self.TRAIN_ENTITIES[article_index]\n",
    "            for entity_index in range(4):\n",
    "                for query_index in range(len(article)):\n",
    "                    query = article[query_index]\n",
    "                    for supp_index in range(len(query)):\n",
    "                        orig_entity = query[0][entity_index].strip().lower()\n",
    "                        entity = query[supp_index][entity_index].strip().lower()\n",
    "                        gold_ent = train_identifiers[article_index][entity_index].strip().lower() \n",
    "                        if gold_ent == \"\" or gold_ent == 'zero':\n",
    "                            continue\n",
    "\n",
    "                        if entity == orig_entity:\n",
    "                            continue\n",
    "                        if entity_index > 0: #not shooter\n",
    "                            if entity == gold_ent:\n",
    "                                count[entity_index] += 1\n",
    "                        else:\n",
    "                            orig_entity = set(orig_entity.split('|'))\n",
    "                            gold = set(gold_ent.split('|'))\n",
    "                            entity = set(entity.split('|'))\n",
    "                            if len(entity.intersection(gold)) > len(orig_entity.intersection(gold)):\n",
    "                                count[entity_index] += 1                            \n",
    "                        total_count[entity_index] +=1\n",
    "\n",
    "#         print \"COUNT \", count\n",
    "#         print \"TOTAL \", total_count\n",
    "        print \"ENTS: counts gold not in orig\" , [count[i] for i in range(4)]\n",
    "        print \"Ratio\" , [count[i]/total_count[i] for i in range(4)]\n",
    "\n",
    "        print \"TEST: Exploring if classifier ever chooses not first entity\"\n",
    "        ones_not_orig = [0] * 4\n",
    "        ones = [0] * 4\n",
    "        counts = [0] * 4\n",
    "        for entity_index in range(4):\n",
    "            for article_index in range(len(self.TEST_ENTITIES)):\n",
    "                article = self.TEST_ENTITIES[article_index]\n",
    "                for query_index in range(len(article)):\n",
    "                    query = article[query_index]\n",
    "                    orig_entity = query[0][entity_index].strip().lower()\n",
    "                    gold = test_identifiers[article_index][entity_index].strip().lower()\n",
    "                    for supp_index in range(len(query)):\n",
    "                        decision = DECISIONS[article_index][query_index][supp_index][entity_index]\n",
    "                        counts[entity_index] += 1\n",
    "                        if decision == 1:\n",
    "                            entity = query[supp_index][entity_index].strip().lower()\n",
    "                            ones[entity_index] += 1\n",
    "                            if not entity == orig_entity and not entity == \"\":\n",
    "                                ones_not_orig[entity_index] += 1\n",
    "\n",
    "        print \"DECS: counts Chosen not in orig\" , ones_not_orig\n",
    "        print \"DECS: counts ONES \" , ones\n",
    "        print \"Ratio one not matching original entity in prediction\", [ ones_not_orig[x]*1. / counts[x] for x in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 292\n",
      "Class dist [124, 170, 193, 260, 0, 1000]\n",
      "Total labels 1747\n",
      "TRAIN:: Exploring how many times gold entity is not in original document\n",
      "ENTS: counts gold not in orig [4.0, 140.0, 93.0, 260.0]\n",
      "Ratio [0.0021287919105907396, 0.041716328963051254, 0.015361744301288404, 0.036775106082036775]\n",
      "TEST: Exploring if classifier ever chooses not first entity\n",
      "DECS: counts Chosen not in orig [23, 176, 274, 251]\n",
      "DECS: counts ONES  [1484, 1636, 1734, 1713]\n",
      "Ratio one not matching original entity in prediction [0.0024562152926099956, 0.018795386586928663, 0.02926099957283212, 0.02680478428022213]\n",
      "#############################################################\n",
      "Evaluation for Classifier baseline with MAJORITY aggregation\n",
      "\n",
      "Entity 0 :\n",
      "PRECISION 0.447619047619 RECALL 0.447619047619 F1 0.447619047619\n",
      "Total match 94.0\n",
      "Entity 1 :\n",
      "PRECISION 0.704938271605 RECALL 0.704938271605 F1 0.704938271605\n",
      "Total match 571.0\n",
      "Entity 2 :\n",
      "PRECISION 0.670848708487 RECALL 0.670848708487 F1 0.670848708487\n",
      "Total match 909.0\n",
      "Entity 3 :\n",
      "PRECISION 0.549315068493 RECALL 0.549315068493 F1 0.549315068493\n",
      "Total match 802.0\n",
      "\n",
      "#############################################################\n",
      "Evaluation for Classifier baseline with MAX CONFIDENCE aggregation\n",
      "\n",
      "Entity 0 :\n",
      "PRECISION 0.452380952381 RECALL 0.452380952381 F1 0.452380952381\n",
      "Total match 95.0\n",
      "Entity 1 :\n",
      "PRECISION 0.707407407407 RECALL 0.707407407407 F1 0.707407407407\n",
      "Total match 573.0\n",
      "Entity 2 :\n",
      "PRECISION 0.684132841328 RECALL 0.684132841328 F1 0.684132841328\n",
      "Total match 927.0\n",
      "Entity 3 :\n",
      "PRECISION 0.552739726027 RECALL 0.552739726027 F1 0.552739726027\n",
      "Total match 807.0\n",
      "\n",
      "#############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "\n",
    "count_name = 0\n",
    "print \"size\", len(TEST_CONFIDENCES_CRF)\n",
    "\n",
    "for entity_ind in range(4):\n",
    "    entity_name = p.int2tags[entity_ind+1]\n",
    "    correct = 0\n",
    "    gold_num = 0\n",
    "    total = 0\n",
    "    for article_ind in range(len(TEST_ENTITIES_CRF)):\n",
    "        article = TEST_ENTITIES_CRF[article_ind]\n",
    "        gold_ent = test_identifiers[article_ind][entity_ind].strip().lower()\n",
    "        for query_ind in range(len(article)):\n",
    "            query = article[query_ind]\n",
    "#             for sup_ind in range(len(query)):\n",
    "                \n",
    "\n",
    "            sup = query[0]\n",
    "\n",
    "            ent = sup[entity_ind].strip().lower()\n",
    "            \n",
    "            if gold_ent == '' or (True and gold_ent == 'zero'):\n",
    "                    continue\n",
    "            if entity_ind == 0:\n",
    "                    gold_ent_set = set(gold_ent.split('|'))\n",
    "                    correct_int = gold_ent_set.intersection(ent)\n",
    "                    correct += (1 if len(correct_int)>0 else 0)\n",
    "                    if not ent == set(['']):\n",
    "                        count_name += 1\n",
    "                    gold_num += 1\n",
    "                    total += 1\n",
    "            else:\n",
    "\n",
    "                if ent == gold_ent: \n",
    "                    correct += 1\n",
    "                gold_num += 1\n",
    "                total   += 1\n",
    "\n",
    "    prec = correct*1./total\n",
    "    recall = correct*1./gold_num\n",
    "    f1 = 0\n",
    "    if not prec + recall == 0:\n",
    "        f1 = 2*(prec*recall)/(prec+recall)\n",
    "#     print entity_name\n",
    "#     print \"prec\", prec, \"recall\", recall, \"f1:\" , f1\n",
    "            \n",
    "# baseline = Classifier(TRAIN_ENTITIES_CRF, TRAIN_CONFIDENCES_CRF, TRAIN_COSINE_SIM_CRF,\\\n",
    "#              TEST_ENTITIES_CRF, TEST_CONFIDENCES_CRF, TEST_COSINE_SIM_CRF)\n",
    "   \n",
    "baseline = Classifier(TRAIN_ENTITIES, TRAIN_CONFIDENCES, TRAIN_COSINE_SIM, \\\n",
    "                      TEST_ENTITIES, TEST_CONFIDENCES, TEST_COSINE_SIM)\n",
    "            \n",
    "                                                                                                                                                                                                    \n",
    "        \n",
    "baseline.trainAndEval(train_identifiers, test_identifiers, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-459-3678b1ed92bd>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-459-3678b1ed92bd>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    shooterName f1: 0.452380952381\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print sklearn.__version__\n",
    "\n",
    "OLD\n",
    "shooterName f1: 0.452380952381aka 45.2\n",
    "killedNum f1: 0.697530864198  aka 69.8\n",
    "woundedNum f1: 0.686346863469 aka 68.6\n",
    "city f1: 0.537671232877       aka 53.8\n",
    "    \n",
    "    \n",
    "vs:\n",
    "NEW    \n",
    "shooterName F1 0.452380952381 aka 45.2   diff 0\n",
    "killedNum   F1 0.707407407407 aka 70.7   diff +0.9\n",
    "woundedNum  F1 0.684132841328 aka 68.4   diff -0.2\n",
    "city F1 0.552739726027        aka 55.3   diff +1.5\n",
    "                                         net  +2.2\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
